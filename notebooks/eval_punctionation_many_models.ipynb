{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "116b2e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ac3a072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from life_after_bert import load_olmpics_data, MCDataset, evaluate_encoder, evaluate_decoder, evaluate_encoder_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1390efea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class PunctuationDataset(Dataset):  # TODO: move from notebook to src\n",
    "    \"\"\" Misnomer, assumes periods and not question marks \"\"\"\n",
    "    def __init__(self, questions, choices, answer_ids, tokenizer, mask_token=None, max_length=25, punctuation=True):\n",
    "        mask_token = mask_token if mask_token is not None else tokenizer.mask_token\n",
    "        assert mask_token is not None, \"mask_token must be provided if tokenizer.mask_token does not exist\"\n",
    "        questions = [question.replace(\"[MASK]\", mask_token).strip(\" \") for question in questions]\n",
    "        if not punctuation:\n",
    "            questions = [question.strip(\".\").strip(\" \") for question in questions]  # Some examples have space before period\n",
    "        else:\n",
    "            questions = [question if question.endswith(\".\") else f\"{question}.\" for question in questions]\n",
    "        \n",
    "        out = tokenizer(questions, max_length=max_length, padding=\"max_length\")\n",
    "        self.input_ids = out[\"input_ids\"]\n",
    "        self.attention_mask = out[\"attention_mask\"]\n",
    "        self.questions = questions\n",
    "        self.choices = choices\n",
    "        self.answer_ids = answer_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[i],\n",
    "            \"attention_mask\": self.attention_mask[i],\n",
    "            \"choice_list\": self.choices[i],\n",
    "            \"answer_id\": self.answer_ids[i],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b63cd364",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [(\"bert-base-uncased\", None), (\"bert-large-uncased\", None), (\"bert-large-uncased-whole-word-masking\", None), \n",
    "               (\"roberta-large\", None), (\"distilbert-base-uncased\", None), (\"albert-base-v1\", None), \n",
    "               (\"albert-large-v1\", None), (\"albert-xlarge-v1\", None), (\"albert-xxlarge-v1\", None), \n",
    "               (\"albert-base-v2\", None), (\"albert-large-v2\", None), (\"albert-xlarge-v2\", None), \n",
    "               (\"albert-xxlarge-v2\", None), (\"gpt2-large\", \"[MASK]\"), (\"t5-small\", \"<extra_id_0>\"), \n",
    "               (\"t5-base\", \"<extra_id_0>\"), (\"t5-large\", \"<extra_id_0>\"), (\"t5-3b\", \"<extra_id_0>\"), \n",
    "               (\"google/t5-v1_1-small\", \"<extra_id_0>\"), (\"google/t5-v1_1-base\", \"<extra_id_0>\"), \n",
    "               (\"google/t5-v1_1-large\", \"<extra_id_0>\"), (\"google/t5-v1_1-xl\", \"<extra_id_0>\")]\n",
    "\n",
    "def get_model_type(model_name):\n",
    "    if \"t5\" in model_name:\n",
    "        return \"t5\"\n",
    "    if \"gpt\" in model_name:\n",
    "        return \"decoder\"\n",
    "    if \"bert\" in model_name:\n",
    "        return \"encoder\"\n",
    "    \n",
    "    raise NotImplementedError\n",
    "\n",
    "task = \"oLMpics MLM\"\n",
    "\n",
    "eval_datasets = [(\"age_comparison\", 2), (\"always_never\", 5), (\"size_comparison\", 2), \n",
    "                 (\"antonym_negation\", 2), (\"taxonomy_conjunction\", 3), (\"multihop_composition\", 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f251f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_accs_dict = {}\n",
    "for model_name, mask_token in model_names:\n",
    "    model_type = get_model_type(model_name)\n",
    "    if model_type == \"t5\":\n",
    "        model = transformers.T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "    elif model_type == \"encoder\":\n",
    "        model = transformers.AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "    elif model_type == \"decoder\":\n",
    "        model = transformers.AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    if mask_token is not None:\n",
    "        tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, mask_token=mask_token)\n",
    "    else:\n",
    "        tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "    if tokenizer.pad_token == None:\n",
    "        print(\"Defaulting pad token to EOS token.\")\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    all_accs = []\n",
    "    for data_path, num_choices in eval_datasets:\n",
    "        max_length = 26 if data_path == \"taxonomy_conjunction\" else 25\n",
    "        questions, choice_lists, answer_ids = load_olmpics_data(f\"../tests/data/oLMpics_{data_path}_dev.jsonl\", num_choices, progress_bar=False)\n",
    "\n",
    "        acc_list = []\n",
    "        for punctuation in [True, False]:\n",
    "            dataset = PunctuationDataset(questions, choice_lists, answer_ids, tokenizer, max_length=max_length, punctuation=punctuation)\n",
    "\n",
    "            if model_type == \"t5\":\n",
    "                decoder_prompt = tokenizer(\"<pad> <extra_id_0>\", add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "                all_answers, all_preds = evaluate_encoder_decoder(model, tokenizer, task, dataset, decoder_prompt, device, progress_bar=False)\n",
    "            elif model_type == \"encoder\":\n",
    "                all_answers, all_preds = evaluate_encoder(model, tokenizer, task, dataset, device, progress_bar=False)\n",
    "            elif model_type == \"decoder\":\n",
    "                all_answers, all_preds = evaluate_decoder(model, tokenizer, task, num_choices, dataset, device, progress_bar=False)\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "            acc_list.append((np.array(all_answers) == np.array(all_preds)).mean())\n",
    "\n",
    "        all_accs.append((data_path, acc_list))\n",
    "\n",
    "    model_accs_dict = {}\n",
    "    print(f\"Accuracy for {model_name}:\")\n",
    "    for task_name, accs in all_accs:\n",
    "        model_accs_dict[task_name] = accs\n",
    "        print(f\"Task: {task_name} \\t Accuracy with punctuation: {accs[0]} \\t Accuracy without punctuation: {accs[1]}\")\n",
    "        \n",
    "    all_accs_dict[model_name] = model_accs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c56a1edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for bert-base-uncased:\n",
      "Task: age_comparison \t Accuracy with punctuation: 0.494 \t Accuracy without punctuation: 0.494\n",
      "Task: always_never \t Accuracy with punctuation: 0.13214285714285715 \t Accuracy without punctuation: 0.1\n",
      "Task: size_comparison \t Accuracy with punctuation: 0.554 \t Accuracy without punctuation: 0.556\n",
      "Task: antonym_negation \t Accuracy with punctuation: 0.538 \t Accuracy without punctuation: 0.532\n",
      "Task: taxonomy_conjunction \t Accuracy with punctuation: 0.44073455759599334 \t Accuracy without punctuation: 0.4674457429048414\n",
      "Task: multihop_composition \t Accuracy with punctuation: 0.322 \t Accuracy without punctuation: 0.332\n",
      "\n",
      "\n",
      "Accuracy for bert-large-uncased:\n",
      "Task: age_comparison \t Accuracy with punctuation: 0.506 \t Accuracy without punctuation: 0.506\n",
      "Task: always_never \t Accuracy with punctuation: 0.225 \t Accuracy without punctuation: 0.26071428571428573\n",
      "Task: size_comparison \t Accuracy with punctuation: 0.526 \t Accuracy without punctuation: 0.55\n",
      "Task: antonym_negation \t Accuracy with punctuation: 0.506 \t Accuracy without punctuation: 0.518\n",
      "Task: taxonomy_conjunction \t Accuracy with punctuation: 0.44240400667779634 \t Accuracy without punctuation: 0.5392320534223706\n",
      "Task: multihop_composition \t Accuracy with punctuation: 0.338 \t Accuracy without punctuation: 0.338\n",
      "\n",
      "\n",
      "Accuracy for bert-large-uncased-whole-word-masking:\n",
      "Task: age_comparison \t Accuracy with punctuation: 0.764 \t Accuracy without punctuation: 0.832\n",
      "Task: always_never \t Accuracy with punctuation: 0.10714285714285714 \t Accuracy without punctuation: 0.15357142857142858\n",
      "Task: size_comparison \t Accuracy with punctuation: 0.558 \t Accuracy without punctuation: 0.6\n",
      "Task: antonym_negation \t Accuracy with punctuation: 0.572 \t Accuracy without punctuation: 0.57\n",
      "Task: taxonomy_conjunction \t Accuracy with punctuation: 0.4540901502504174 \t Accuracy without punctuation: 0.46410684474123537\n",
      "Task: multihop_composition \t Accuracy with punctuation: 0.338 \t Accuracy without punctuation: 0.338\n",
      "\n",
      "\n",
      "Accuracy for roberta-large:\n",
      "Task: age_comparison \t Accuracy with punctuation: 0.986 \t Accuracy without punctuation: 0.978\n",
      "Task: always_never \t Accuracy with punctuation: 0.1357142857142857 \t Accuracy without punctuation: 0.16071428571428573\n",
      "Task: size_comparison \t Accuracy with punctuation: 0.874 \t Accuracy without punctuation: 0.836\n",
      "Task: antonym_negation \t Accuracy with punctuation: 0.746 \t Accuracy without punctuation: 0.664\n",
      "Task: taxonomy_conjunction \t Accuracy with punctuation: 0.43906510851419034 \t Accuracy without punctuation: 0.4607679465776294\n",
      "Task: multihop_composition \t Accuracy with punctuation: 0.296 \t Accuracy without punctuation: 0.29\n",
      "\n",
      "\n",
      "Accuracy for distilbert-base-uncased:\n",
      "Task: age_comparison \t Accuracy with punctuation: 0.494 \t Accuracy without punctuation: 0.492\n",
      "Task: always_never \t Accuracy with punctuation: 0.15 \t Accuracy without punctuation: 0.11428571428571428\n",
      "Task: size_comparison \t Accuracy with punctuation: 0.508 \t Accuracy without punctuation: 0.518\n",
      "Task: antonym_negation \t Accuracy with punctuation: 0.508 \t Accuracy without punctuation: 0.516\n",
      "Task: taxonomy_conjunction \t Accuracy with punctuation: 0.4507512520868113 \t Accuracy without punctuation: 0.4691151919866444\n",
      "Task: multihop_composition \t Accuracy with punctuation: 0.34 \t Accuracy without punctuation: 0.34\n",
      "\n",
      "\n",
      "Accuracy for albert-base-v1:\n",
      "Task: age_comparison \t Accuracy with punctuation: 0.468 \t Accuracy without punctuation: 0.5\n",
      "Task: always_never \t Accuracy with punctuation: 0.23214285714285715 \t Accuracy without punctuation: 0.17857142857142858\n",
      "Task: size_comparison \t Accuracy with punctuation: 0.506 \t Accuracy without punctuation: 0.506\n",
      "Task: antonym_negation \t Accuracy with punctuation: 0.526 \t Accuracy without punctuation: 0.512\n",
      "Task: taxonomy_conjunction \t Accuracy with punctuation: 0.5258764607679466 \t Accuracy without punctuation: 0.5459098497495827\n",
      "Task: multihop_composition \t Accuracy with punctuation: 0.34 \t Accuracy without punctuation: 0.34\n",
      "\n",
      "\n",
      "Accuracy for albert-large-v1:\n",
      "Task: age_comparison \t Accuracy with punctuation: 0.53 \t Accuracy without punctuation: 0.514\n",
      "Task: always_never \t Accuracy with punctuation: 0.30714285714285716 \t Accuracy without punctuation: 0.21785714285714286\n",
      "Task: size_comparison \t Accuracy with punctuation: 0.492 \t Accuracy without punctuation: 0.492\n",
      "Task: antonym_negation \t Accuracy with punctuation: 0.502 \t Accuracy without punctuation: 0.51\n",
      "Task: taxonomy_conjunction \t Accuracy with punctuation: 0.5091819699499165 \t Accuracy without punctuation: 0.5191986644407346\n",
      "Task: multihop_composition \t Accuracy with punctuation: 0.34 \t Accuracy without punctuation: 0.34\n",
      "\n",
      "\n",
      "Accuracy for albert-xlarge-v1:\n",
      "Task: age_comparison \t Accuracy with punctuation: 0.398 \t Accuracy without punctuation: 0.494\n",
      "Task: always_never \t Accuracy with punctuation: 0.26071428571428573 \t Accuracy without punctuation: 0.2357142857142857\n",
      "Task: size_comparison \t Accuracy with punctuation: 0.504 \t Accuracy without punctuation: 0.502\n",
      "Task: antonym_negation \t Accuracy with punctuation: 0.446 \t Accuracy without punctuation: 0.454\n",
      "Task: taxonomy_conjunction \t Accuracy with punctuation: 0.49248747913188645 \t Accuracy without punctuation: 0.5242070116861436\n",
      "Task: multihop_composition \t Accuracy with punctuation: 0.318 \t Accuracy without punctuation: 0.322\n",
      "\n",
      "\n",
      "Accuracy for albert-xxlarge-v1:\n",
      "Task: age_comparison \t Accuracy with punctuation: 0.954 \t Accuracy without punctuation: 0.828\n",
      "Task: always_never \t Accuracy with punctuation: 0.22857142857142856 \t Accuracy without punctuation: 0.225\n",
      "Task: size_comparison \t Accuracy with punctuation: 0.612 \t Accuracy without punctuation: 0.56\n",
      "Task: antonym_negation \t Accuracy with punctuation: 0.664 \t Accuracy without punctuation: 0.564\n",
      "Task: taxonomy_conjunction \t Accuracy with punctuation: 0.4607679465776294 \t Accuracy without punctuation: 0.5158597662771286\n",
      "Task: multihop_composition \t Accuracy with punctuation: 0.418 \t Accuracy without punctuation: 0.34\n",
      "\n",
      "\n",
      "Accuracy for albert-base-v2:\n",
      "Task: age_comparison \t Accuracy with punctuation: 0.506 \t Accuracy without punctuation: 0.506\n",
      "Task: always_never \t Accuracy with punctuation: 0.21428571428571427 \t Accuracy without punctuation: 0.16428571428571428\n",
      "Task: size_comparison \t Accuracy with punctuation: 0.494 \t Accuracy without punctuation: 0.494\n",
      "Task: antonym_negation \t Accuracy with punctuation: 0.54 \t Accuracy without punctuation: 0.504\n",
      "Task: taxonomy_conjunction \t Accuracy with punctuation: 0.4958263772954925 \t Accuracy without punctuation: 0.5091819699499165\n",
      "Task: multihop_composition \t Accuracy with punctuation: 0.34 \t Accuracy without punctuation: 0.34\n",
      "\n",
      "\n",
      "Accuracy for albert-large-v2:\n",
      "Task: age_comparison \t Accuracy with punctuation: 0.512 \t Accuracy without punctuation: 0.516\n",
      "Task: always_never \t Accuracy with punctuation: 0.3107142857142857 \t Accuracy without punctuation: 0.35\n",
      "Task: size_comparison \t Accuracy with punctuation: 0.506 \t Accuracy without punctuation: 0.522\n",
      "Task: antonym_negation \t Accuracy with punctuation: 0.552 \t Accuracy without punctuation: 0.456\n",
      "Task: taxonomy_conjunction \t Accuracy with punctuation: 0.5108514190317195 \t Accuracy without punctuation: 0.4757929883138564\n",
      "Task: multihop_composition \t Accuracy with punctuation: 0.324 \t Accuracy without punctuation: 0.34\n",
      "\n",
      "\n",
      "Accuracy for albert-xlarge-v2:\n",
      "Task: age_comparison \t Accuracy with punctuation: 0.462 \t Accuracy without punctuation: 0.48\n",
      "Task: always_never \t Accuracy with punctuation: 0.37857142857142856 \t Accuracy without punctuation: 0.35\n",
      "Task: size_comparison \t Accuracy with punctuation: 0.506 \t Accuracy without punctuation: 0.506\n",
      "Task: antonym_negation \t Accuracy with punctuation: 0.624 \t Accuracy without punctuation: 0.496\n",
      "Task: taxonomy_conjunction \t Accuracy with punctuation: 0.49248747913188645 \t Accuracy without punctuation: 0.5475792988313857\n",
      "Task: multihop_composition \t Accuracy with punctuation: 0.326 \t Accuracy without punctuation: 0.324\n",
      "\n",
      "\n",
      "Accuracy for albert-xxlarge-v2:\n",
      "Task: age_comparison \t Accuracy with punctuation: 0.938 \t Accuracy without punctuation: 0.818\n",
      "Task: always_never \t Accuracy with punctuation: 0.2392857142857143 \t Accuracy without punctuation: 0.23214285714285715\n",
      "Task: size_comparison \t Accuracy with punctuation: 0.788 \t Accuracy without punctuation: 0.62\n",
      "Task: antonym_negation \t Accuracy with punctuation: 0.648 \t Accuracy without punctuation: 0.506\n",
      "Task: taxonomy_conjunction \t Accuracy with punctuation: 0.4357262103505843 \t Accuracy without punctuation: 0.4941569282136895\n",
      "Task: multihop_composition \t Accuracy with punctuation: 0.386 \t Accuracy without punctuation: 0.34\n",
      "\n",
      "\n",
      "Accuracy for gpt2-large:\n",
      "Task: age_comparison \t Accuracy with punctuation: 0.696 \t Accuracy without punctuation: 0.7\n",
      "Task: always_never \t Accuracy with punctuation: 0.2571428571428571 \t Accuracy without punctuation: 0.16785714285714284\n",
      "Task: size_comparison \t Accuracy with punctuation: 0.508 \t Accuracy without punctuation: 0.506\n",
      "Task: antonym_negation \t Accuracy with punctuation: 0.586 \t Accuracy without punctuation: 0.568\n",
      "Task: taxonomy_conjunction \t Accuracy with punctuation: 0.4373956594323873 \t Accuracy without punctuation: 0.4290484140233723\n",
      "Task: multihop_composition \t Accuracy with punctuation: 0.32 \t Accuracy without punctuation: 0.338\n",
      "\n",
      "\n",
      "Accuracy for t5-small:\n",
      "Task: age_comparison \t Accuracy with punctuation: 0.494 \t Accuracy without punctuation: 0.494\n",
      "Task: always_never \t Accuracy with punctuation: 0.16071428571428573 \t Accuracy without punctuation: 0.14285714285714285\n",
      "Task: size_comparison \t Accuracy with punctuation: 0.482 \t Accuracy without punctuation: 0.49\n",
      "Task: antonym_negation \t Accuracy with punctuation: 0.47 \t Accuracy without punctuation: 0.5\n",
      "Task: taxonomy_conjunction \t Accuracy with punctuation: 0.4774624373956594 \t Accuracy without punctuation: 0.49248747913188645\n",
      "Task: multihop_composition \t Accuracy with punctuation: 0.356 \t Accuracy without punctuation: 0.342\n",
      "\n",
      "\n",
      "Accuracy for t5-base:\n",
      "Task: age_comparison \t Accuracy with punctuation: 0.494 \t Accuracy without punctuation: 0.494\n",
      "Task: always_never \t Accuracy with punctuation: 0.10714285714285714 \t Accuracy without punctuation: 0.175\n",
      "Task: size_comparison \t Accuracy with punctuation: 0.592 \t Accuracy without punctuation: 0.606\n",
      "Task: antonym_negation \t Accuracy with punctuation: 0.534 \t Accuracy without punctuation: 0.47\n",
      "Task: taxonomy_conjunction \t Accuracy with punctuation: 0.4941569282136895 \t Accuracy without punctuation: 0.4674457429048414\n",
      "Task: multihop_composition \t Accuracy with punctuation: 0.312 \t Accuracy without punctuation: 0.324\n",
      "\n",
      "\n",
      "Accuracy for t5-large:\n",
      "Task: age_comparison \t Accuracy with punctuation: 0.94 \t Accuracy without punctuation: 0.956\n",
      "Task: always_never \t Accuracy with punctuation: 0.2571428571428571 \t Accuracy without punctuation: 0.18928571428571428\n",
      "Task: size_comparison \t Accuracy with punctuation: 0.832 \t Accuracy without punctuation: 0.85\n",
      "Task: antonym_negation \t Accuracy with punctuation: 0.646 \t Accuracy without punctuation: 0.658\n",
      "Task: taxonomy_conjunction \t Accuracy with punctuation: 0.46410684474123537 \t Accuracy without punctuation: 0.42237061769616024\n",
      "Task: multihop_composition \t Accuracy with punctuation: 0.338 \t Accuracy without punctuation: 0.338\n",
      "\n",
      "\n",
      "Accuracy for t5-3b:\n",
      "Task: age_comparison \t Accuracy with punctuation: 1.0 \t Accuracy without punctuation: 0.996\n",
      "Task: always_never \t Accuracy with punctuation: 0.20357142857142857 \t Accuracy without punctuation: 0.2392857142857143\n",
      "Task: size_comparison \t Accuracy with punctuation: 0.9 \t Accuracy without punctuation: 0.906\n",
      "Task: antonym_negation \t Accuracy with punctuation: 0.684 \t Accuracy without punctuation: 0.702\n",
      "Task: taxonomy_conjunction \t Accuracy with punctuation: 0.4307178631051753 \t Accuracy without punctuation: 0.41235392320534225\n",
      "Task: multihop_composition \t Accuracy with punctuation: 0.346 \t Accuracy without punctuation: 0.362\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_name in all_accs_dict.keys():\n",
    "    print(f\"Accuracy for {model_name}:\")\n",
    "    for task_name, accs in all_accs_dict[model_name].items():\n",
    "        print(f\"Task: {task_name} \\t Accuracy with punctuation: {accs[0]} \\t Accuracy without punctuation: {accs[1]}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b409f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bert-base-uncased': {'age_comparison': [0.494, 0.494],\n",
       "  'always_never': [0.13214285714285715, 0.1],\n",
       "  'size_comparison': [0.554, 0.556],\n",
       "  'antonym_negation': [0.538, 0.532],\n",
       "  'taxonomy_conjunction': [0.44073455759599334, 0.4674457429048414],\n",
       "  'multihop_composition': [0.322, 0.332]},\n",
       " 'bert-large-uncased': {'age_comparison': [0.506, 0.506],\n",
       "  'always_never': [0.225, 0.26071428571428573],\n",
       "  'size_comparison': [0.526, 0.55],\n",
       "  'antonym_negation': [0.506, 0.518],\n",
       "  'taxonomy_conjunction': [0.44240400667779634, 0.5392320534223706],\n",
       "  'multihop_composition': [0.338, 0.338]},\n",
       " 'bert-large-uncased-whole-word-masking': {'age_comparison': [0.764, 0.832],\n",
       "  'always_never': [0.10714285714285714, 0.15357142857142858],\n",
       "  'size_comparison': [0.558, 0.6],\n",
       "  'antonym_negation': [0.572, 0.57],\n",
       "  'taxonomy_conjunction': [0.4540901502504174, 0.46410684474123537],\n",
       "  'multihop_composition': [0.338, 0.338]},\n",
       " 'roberta-large': {'age_comparison': [0.986, 0.978],\n",
       "  'always_never': [0.1357142857142857, 0.16071428571428573],\n",
       "  'size_comparison': [0.874, 0.836],\n",
       "  'antonym_negation': [0.746, 0.664],\n",
       "  'taxonomy_conjunction': [0.43906510851419034, 0.4607679465776294],\n",
       "  'multihop_composition': [0.296, 0.29]},\n",
       " 'distilbert-base-uncased': {'age_comparison': [0.494, 0.492],\n",
       "  'always_never': [0.15, 0.11428571428571428],\n",
       "  'size_comparison': [0.508, 0.518],\n",
       "  'antonym_negation': [0.508, 0.516],\n",
       "  'taxonomy_conjunction': [0.4507512520868113, 0.4691151919866444],\n",
       "  'multihop_composition': [0.34, 0.34]},\n",
       " 'albert-base-v1': {'age_comparison': [0.468, 0.5],\n",
       "  'always_never': [0.23214285714285715, 0.17857142857142858],\n",
       "  'size_comparison': [0.506, 0.506],\n",
       "  'antonym_negation': [0.526, 0.512],\n",
       "  'taxonomy_conjunction': [0.5258764607679466, 0.5459098497495827],\n",
       "  'multihop_composition': [0.34, 0.34]},\n",
       " 'albert-large-v1': {'age_comparison': [0.53, 0.514],\n",
       "  'always_never': [0.30714285714285716, 0.21785714285714286],\n",
       "  'size_comparison': [0.492, 0.492],\n",
       "  'antonym_negation': [0.502, 0.51],\n",
       "  'taxonomy_conjunction': [0.5091819699499165, 0.5191986644407346],\n",
       "  'multihop_composition': [0.34, 0.34]},\n",
       " 'albert-xlarge-v1': {'age_comparison': [0.398, 0.494],\n",
       "  'always_never': [0.26071428571428573, 0.2357142857142857],\n",
       "  'size_comparison': [0.504, 0.502],\n",
       "  'antonym_negation': [0.446, 0.454],\n",
       "  'taxonomy_conjunction': [0.49248747913188645, 0.5242070116861436],\n",
       "  'multihop_composition': [0.318, 0.322]},\n",
       " 'albert-xxlarge-v1': {'age_comparison': [0.954, 0.828],\n",
       "  'always_never': [0.22857142857142856, 0.225],\n",
       "  'size_comparison': [0.612, 0.56],\n",
       "  'antonym_negation': [0.664, 0.564],\n",
       "  'taxonomy_conjunction': [0.4607679465776294, 0.5158597662771286],\n",
       "  'multihop_composition': [0.418, 0.34]},\n",
       " 'albert-base-v2': {'age_comparison': [0.506, 0.506],\n",
       "  'always_never': [0.21428571428571427, 0.16428571428571428],\n",
       "  'size_comparison': [0.494, 0.494],\n",
       "  'antonym_negation': [0.54, 0.504],\n",
       "  'taxonomy_conjunction': [0.4958263772954925, 0.5091819699499165],\n",
       "  'multihop_composition': [0.34, 0.34]},\n",
       " 'albert-large-v2': {'age_comparison': [0.512, 0.516],\n",
       "  'always_never': [0.3107142857142857, 0.35],\n",
       "  'size_comparison': [0.506, 0.522],\n",
       "  'antonym_negation': [0.552, 0.456],\n",
       "  'taxonomy_conjunction': [0.5108514190317195, 0.4757929883138564],\n",
       "  'multihop_composition': [0.324, 0.34]},\n",
       " 'albert-xlarge-v2': {'age_comparison': [0.462, 0.48],\n",
       "  'always_never': [0.37857142857142856, 0.35],\n",
       "  'size_comparison': [0.506, 0.506],\n",
       "  'antonym_negation': [0.624, 0.496],\n",
       "  'taxonomy_conjunction': [0.49248747913188645, 0.5475792988313857],\n",
       "  'multihop_composition': [0.326, 0.324]},\n",
       " 'albert-xxlarge-v2': {'age_comparison': [0.938, 0.818],\n",
       "  'always_never': [0.2392857142857143, 0.23214285714285715],\n",
       "  'size_comparison': [0.788, 0.62],\n",
       "  'antonym_negation': [0.648, 0.506],\n",
       "  'taxonomy_conjunction': [0.4357262103505843, 0.4941569282136895],\n",
       "  'multihop_composition': [0.386, 0.34]},\n",
       " 'gpt2-large': {'age_comparison': [0.696, 0.7],\n",
       "  'always_never': [0.2571428571428571, 0.16785714285714284],\n",
       "  'size_comparison': [0.508, 0.506],\n",
       "  'antonym_negation': [0.586, 0.568],\n",
       "  'taxonomy_conjunction': [0.4373956594323873, 0.4290484140233723],\n",
       "  'multihop_composition': [0.32, 0.338]},\n",
       " 't5-small': {'age_comparison': [0.494, 0.494],\n",
       "  'always_never': [0.16071428571428573, 0.14285714285714285],\n",
       "  'size_comparison': [0.482, 0.49],\n",
       "  'antonym_negation': [0.47, 0.5],\n",
       "  'taxonomy_conjunction': [0.4774624373956594, 0.49248747913188645],\n",
       "  'multihop_composition': [0.356, 0.342]},\n",
       " 't5-base': {'age_comparison': [0.494, 0.494],\n",
       "  'always_never': [0.10714285714285714, 0.175],\n",
       "  'size_comparison': [0.592, 0.606],\n",
       "  'antonym_negation': [0.534, 0.47],\n",
       "  'taxonomy_conjunction': [0.4941569282136895, 0.4674457429048414],\n",
       "  'multihop_composition': [0.312, 0.324]},\n",
       " 't5-large': {'age_comparison': [0.94, 0.956],\n",
       "  'always_never': [0.2571428571428571, 0.18928571428571428],\n",
       "  'size_comparison': [0.832, 0.85],\n",
       "  'antonym_negation': [0.646, 0.658],\n",
       "  'taxonomy_conjunction': [0.46410684474123537, 0.42237061769616024],\n",
       "  'multihop_composition': [0.338, 0.338]},\n",
       " 't5-3b': {'age_comparison': [1.0, 0.996],\n",
       "  'always_never': [0.20357142857142857, 0.2392857142857143],\n",
       "  'size_comparison': [0.9, 0.906],\n",
       "  'antonym_negation': [0.684, 0.702],\n",
       "  'taxonomy_conjunction': [0.4307178631051753, 0.41235392320534225],\n",
       "  'multihop_composition': [0.346, 0.362]}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_accs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b9e239",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
